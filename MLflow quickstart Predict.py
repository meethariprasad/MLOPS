# Databricks notebook source
# MAGIC %md # MLflow quickstart (Python)
# MAGIC 
# MAGIC With MLflow's autologging capabilities, a single line of code automatically logs the resulting model, the parameters used to create the model, and a model score. MLflow autologging is available for several widely used machine learning packages. This notebook creates a Random Forest model on a simple dataset and uses the the MLflow `autolog()` function to log information generated by the run.
# MAGIC 
# MAGIC For details about what information is logged with `autolog()`, refer to the [MLflow documentation](https://mlflow.org/docs/latest/index.html). 
# MAGIC 
# MAGIC ## Setup
# MAGIC * If you are using a cluster running Databricks Runtime, you must install the mlflow library from PyPI. See Cmd 3.
# MAGIC * If you are using a cluster running Databricks Runtime ML, the mlflow library is already installed. 

# COMMAND ----------

# MAGIC %md Install the mlflow library. 
# MAGIC This is required for Databricks Runtime clusters only. If you are using a cluster running Databricks Runtime ML, skip to Cmd 4. 

# COMMAND ----------

# If you are running Databricks Runtime version 7.1 or above, uncomment this line and run this cell:
#%pip install mlflow

# If you are running Databricks Runtime version 6.4 to 7.0, uncomment this line and run this cell:
#dbutils.library.installPyPI("mlflow")

# COMMAND ----------

# MAGIC %md Import the required libraries.

# COMMAND ----------

import mlflow
import mlflow.sklearn
import pandas as pd
import matplotlib.pyplot as plt

from numpy import savetxt

from sklearn.model_selection import train_test_split
from sklearn.datasets import load_diabetes

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# COMMAND ----------

import mlflow.pyfunc

model_version_uri = "models:/{model_name}/1".format(model_name=model_name)

print("Loading registered model version from URI: '{model_uri}'".format(model_uri=model_version_uri))
model_version_1 = mlflow.pyfunc.load_model(model_version_uri)

model_production_uri = "models:/{model_name}/production".format(model_name=model_name)

print("Loading registered model version from URI: '{model_uri}'".format(model_uri=model_production_uri))
model_production = mlflow.pyfunc.load_model(model_production_uri)

# COMMAND ----------

from sklearn.model_selection import train_test_split
from sklearn.datasets import load_diabetes
db = load_diabetes()
X = db.data
y = db.target
X_train, X_test, y_train, y_test = train_test_split(X, y)

# COMMAND ----------

predicted=model_production.predict(X_test)
import pandas as pd
prediction=pd.DataFrame(predicted,columns=["prediction"])
prediction.to_csv("prediction.csv",index=False)
prediction

# COMMAND ----------

# !/databricks/python_shell/scripts/PythonShell.py configure

# COMMAND ----------

# MAGIC %md
# MAGIC Unit test on predict function: Input vs Output
# MAGIC Input: Tensor (dtype: float64, shape: [-1,10])
# MAGIC Output: Tensor (dtype: float64, shape: [-1])

# COMMAND ----------

# MAGIC %md To view the results, click **Experiment** at the upper right of this page. The Experiments sidebar appears. This sidebar displays the parameters and metrics for each run of this notebook. Click the circular arrows icon to refresh the display to include the latest runs. 
# MAGIC 
# MAGIC When you click the square icon with the arrow to the right of the date and time of the run, the Runs page opens in a new tab. This page shows all of the information that was logged from the run. Scroll down to the Artifacts section to find the logged model.
# MAGIC 
# MAGIC For more information, see View results ([AWS](https://docs.databricks.com/applications/mlflow/quick-start-python.html#view-results)|[Azure](https://docs.microsoft.com/azure/databricks/applications/mlflow/quick-start-python#view-results)|[GCP](https://docs.gcp.databricks.com/applications/mlflow/quick-start-python.html#view-results)).
